{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It feels like just a few days ago it was the l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love my mom . No matter what we go through ,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bump that music ... #imtryingtosleep #sarcasm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mexican and black jokes are pretty much the sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How to find work you love :</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51184</th>\n",
       "      <td>RT My EX had one very annoying habit . Breathi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51185</th>\n",
       "      <td>Some days you're the Titanic , some days you'r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51186</th>\n",
       "      <td>Congrats on the release of 25 @Adele , let's h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51187</th>\n",
       "      <td>doing my favorite thing .. crying #sarcasm</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51188</th>\n",
       "      <td>@TheSupremEC0URT roommate of the week really l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51189 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  labels\n",
       "0      It feels like just a few days ago it was the l...       1\n",
       "1      I love my mom . No matter what we go through ,...       1\n",
       "2          Bump that music ... #imtryingtosleep #sarcasm       1\n",
       "3      Mexican and black jokes are pretty much the sa...       0\n",
       "4                            How to find work you love :       0\n",
       "...                                                  ...     ...\n",
       "51184  RT My EX had one very annoying habit . Breathi...       1\n",
       "51185  Some days you're the Titanic , some days you'r...       1\n",
       "51186  Congrats on the release of 25 @Adele , let's h...       0\n",
       "51187         doing my favorite thing .. crying #sarcasm       1\n",
       "51188  @TheSupremEC0URT roommate of the week really l...       1\n",
       "\n",
       "[51189 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import pandas as pd\n",
    "import logging\n",
    "prefix = 'data/'\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)\n",
    "\n",
    "train_df = pd.read_csv(prefix + 'train.txt', sep='\\t', header=None) \n",
    "train_df = train_df[train_df.columns[1:3]]\n",
    "train_df.columns = ['labels', 'text']\n",
    "train_df = train_df[['text', 'labels']]\n",
    "\n",
    "val_df = pd.read_csv(prefix + 'test.txt', sep='\\t', header=None)\n",
    "val_df = val_df[val_df.columns[1:3]]\n",
    "val_df.columns = ['labels', 'text']\n",
    "val_df = val_df[['text', 'labels']]\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow. I don't know which hurts worse-- my nose or my heart. Well, I'm done speaking to you. Don't be like that. You two need to talk this out. Yeah, 'cause you sound really funny. Sheldon, I'm sorry I didn't tell you about the surgery, but you were worried about nothing. Oh, you're hardly out of the woods, no. You still run the risk of infection, a blood clot, the possibility that an inattentive surgeon let a barn spider lay eggs in your nose. 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When that guy was robbing us and I was locked in the entertainment unit for like six hours, do you know what I was doing there in all that time? I was thinking about how I let you down. Yeah. But if I had known what kind of friend you would turn out to be, I wouldn't have worried about it so much. See you around! We got a box. 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>consider changing disciplines.\\nYeah, to the humanities-- perhaps history. One of the advantages of teaching history is that you don't have to create things. You know, you just have to remember stuff that happened and then parrot it back. You could have fun with that. Yeah, that's not it. Stuart's kind of interested in Amy. Oh, of course he is. She's very interesting. Did you know, when she was 14, she severed the webbing between her own toes? 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Still worried about the money you owe the government? I'm worried about whether Michael Jackson will be able to buy the remains of the elephant man. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey! That monkey has got a Ross on its ass! 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>What are you doing? Key is stuck in the lock. I could fix it hold on hold on, watch out watch out. It still doesn't work! I am not finished. Nice jon Joe, you are quite the craftsman! 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>You liked it? You really liked it? Oh-ho-ho, yeah! Which part exactly? The whole thing! Can we go? 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Supposed to study for my French final with a fourteen year old in the house, its hard enough with an eighty year old. Are you referring to me? Ofcourse no Ma, I am referring to Carrey Grant, he is living in the broom closet. 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>He doesn't need to be sarcastic. I mean, that was sarcasm wasn't it? Really? 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Leonard, what time does your mom's plane get in? I don't know-- some time tomorrow morning. Don't you want to know for sure? No need to. As soon as she flies into California airspace, I'll feel a disturbance in the Force. 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      0\n",
       "0                                                                      Wow. I don't know which hurts worse-- my nose or my heart. Well, I'm done speaking to you. Don't be like that. You two need to talk this out. Yeah, 'cause you sound really funny. Sheldon, I'm sorry I didn't tell you about the surgery, but you were worried about nothing. Oh, you're hardly out of the woods, no. You still run the risk of infection, a blood clot, the possibility that an inattentive surgeon let a barn spider lay eggs in your nose. 0\n",
       "1                                                                                                                                                                                            When that guy was robbing us and I was locked in the entertainment unit for like six hours, do you know what I was doing there in all that time? I was thinking about how I let you down. Yeah. But if I had known what kind of friend you would turn out to be, I wouldn't have worried about it so much. See you around! We got a box. 0\n",
       "2                                                                     consider changing disciplines.\\nYeah, to the humanities-- perhaps history. One of the advantages of teaching history is that you don't have to create things. You know, you just have to remember stuff that happened and then parrot it back. You could have fun with that. Yeah, that's not it. Stuart's kind of interested in Amy. Oh, of course he is. She's very interesting. Did you know, when she was 14, she severed the webbing between her own toes? 1\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                Still worried about the money you owe the government? I'm worried about whether Michael Jackson will be able to buy the remains of the elephant man. 1\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Hey! That monkey has got a Ross on its ass! 1\n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...\n",
       "547                                                                                                                                                                                                                                                                                                                                           What are you doing? Key is stuck in the lock. I could fix it hold on hold on, watch out watch out. It still doesn't work! I am not finished. Nice jon Joe, you are quite the craftsman! 1\n",
       "548                                                                                                                                                                                                                                                                                                                                                                                                                                You liked it? You really liked it? Oh-ho-ho, yeah! Which part exactly? The whole thing! Can we go? 1\n",
       "549                                                                                                                                                                                                                                                                                                  Supposed to study for my French final with a fourteen year old in the house, its hard enough with an eighty year old. Are you referring to me? Ofcourse no Ma, I am referring to Carrey Grant, he is living in the broom closet. 1\n",
       "550                                                                                                                                                                                                                                                                                                                                                                                                                                                      He doesn't need to be sarcastic. I mean, that was sarcasm wasn't it? Really? 1\n",
       "551                                                                                                                                                                                                                                                                                                     Leonard, what time does your mom's plane get in? I don't know-- some time tomorrow morning. Don't you want to know for sure? No need to. As soon as she flies into California airspace, I'll feel a disturbance in the Force. 1\n",
       "\n",
       "[552 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(prefix + 'train_MUSTARD', sep='\\t', header=None) \n",
    "val_df = pd.read_csv(prefix + 'val_MUSTARD', sep='\\t', header=None)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "pd.options.display.max_colwidth = 1000\n",
    "data = json.load(open('data/sarcasm_max512.json'))\n",
    "sarcasm_data = [[' '.join([ *d['context'], d['utterance']]), int(d['sarcasm'])] for d in data.values()]\n",
    "df = pd.DataFrame(sarcasm_data)\n",
    "df.columns = ['labels', 'text']\n",
    "train_df = df.sample(frac = 0.8)\n",
    "val_df = df.drop(train_df.index)\n",
    "with open('data/train_MUSTARD', 'a') as f:\n",
    "    dfAsString = train_df.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)\n",
    "\n",
    "with open('data/val_MUSTARD', 'a') as f:\n",
    "    dfAsString = val_df.to_string(header=False, index=False)\n",
    "    f.write(dfAsString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [labels, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series({c: df[c].map(lambda x: len(str(x))).max() for c in df}).sort_values(ascending =False)\n",
    "mask = (df['labels'].str.len() > 512)\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>And not the good Amazon with one-day shipping....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When are you going to stop making Cylon toast?...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi Joey. Hey! How you doin'? He has the most a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uhh, well I've got an audition down the street...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh you've got to be kidding me. What? As a wed...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Pickin up wedding dresses. Wapah. Whats wapah ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>The audible sigh is a show of exasperation, ri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>So you like the drums! That's, that's great! Y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>What exactly do you think goes on here? Well.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>You heard what he said about his parents. \\nIt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                labels  text\n",
       "0    And not the good Amazon with one-day shipping....     1\n",
       "1    When are you going to stop making Cylon toast?...     1\n",
       "2    Hi Joey. Hey! How you doin'? He has the most a...     0\n",
       "3    Uhh, well I've got an audition down the street...     0\n",
       "4    Oh you've got to be kidding me. What? As a wed...     0\n",
       "..                                                 ...   ...\n",
       "547  Pickin up wedding dresses. Wapah. Whats wapah ...     0\n",
       "548  The audible sigh is a show of exasperation, ri...     1\n",
       "549  So you like the drums! That's, that's great! Y...     1\n",
       "550  What exactly do you think goes on here? Well.....     0\n",
       "551  You heard what he said about his parents. \\nIt...     1\n",
       "\n",
       "[552 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train_MUSTARD.csv')  \n",
    "val_df = pd.read_csv('data/val_MUSTARD.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:simpletransformers.classification.classification_utils: Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f80bbc2bfcd480c9e8bf014bd1b88db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 46\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[39m=\u001b[39m ClassificationModel(\n\u001b[1;32m     42\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mroberta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mroberta-base\u001b[39m\u001b[39m\"\u001b[39m, args\u001b[39m=\u001b[39mmodel_args, use_cuda\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     43\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m model\u001b[39m.\u001b[39;49mtrain_model(train_df)\n\u001b[1;32m     48\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     49\u001b[0m result, model_outputs, wrong_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39meval_model(val_df)\n",
      "File \u001b[0;32m~/anaconda3/envs/irony/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:619\u001b[0m, in \u001b[0;36mClassificationModel.train_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    613\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         train_examples \u001b[39m=\u001b[39m (\n\u001b[1;32m    616\u001b[0m             train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    617\u001b[0m             train_df\u001b[39m.\u001b[39miloc[:, \u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m    618\u001b[0m         )\n\u001b[0;32m--> 619\u001b[0m     train_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_and_cache_examples(\n\u001b[1;32m    620\u001b[0m         train_examples, verbose\u001b[39m=\u001b[39;49mverbose\n\u001b[1;32m    621\u001b[0m     )\n\u001b[1;32m    622\u001b[0m train_sampler \u001b[39m=\u001b[39m RandomSampler(train_dataset)\n\u001b[1;32m    623\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    624\u001b[0m     train_dataset,\n\u001b[1;32m    625\u001b[0m     sampler\u001b[39m=\u001b[39mtrain_sampler,\n\u001b[1;32m    626\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtrain_batch_size,\n\u001b[1;32m    627\u001b[0m     num_workers\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdataloader_num_workers,\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/irony/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:1827\u001b[0m, in \u001b[0;36mClassificationModel.load_and_cache_examples\u001b[0;34m(self, examples, evaluate, no_cache, multi_label, verbose, silent)\u001b[0m\n\u001b[1;32m   1825\u001b[0m         \u001b[39mreturn\u001b[39;00m dataset\n\u001b[1;32m   1826\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1827\u001b[0m     dataset \u001b[39m=\u001b[39m ClassificationDataset(\n\u001b[1;32m   1828\u001b[0m         examples,\n\u001b[1;32m   1829\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m   1830\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m   1831\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   1832\u001b[0m         multi_label\u001b[39m=\u001b[39;49mmulti_label,\n\u001b[1;32m   1833\u001b[0m         output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m   1834\u001b[0m         no_cache\u001b[39m=\u001b[39;49mno_cache,\n\u001b[1;32m   1835\u001b[0m     )\n\u001b[1;32m   1836\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/anaconda3/envs/irony/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:282\u001b[0m, in \u001b[0;36mClassificationDataset.__init__\u001b[0;34m(self, data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, tokenizer, args, mode, multi_label, output_mode, no_cache):\n\u001b[0;32m--> 282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexamples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m build_classification_dataset(\n\u001b[1;32m    283\u001b[0m         data, tokenizer, args, mode, multi_label, output_mode, no_cache\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/irony/lib/python3.10/site-packages/simpletransformers/classification/classification_utils.py:267\u001b[0m, in \u001b[0;36mbuild_classification_dataset\u001b[0;34m(data, tokenizer, args, mode, multi_label, output_mode, no_cache)\u001b[0m\n\u001b[1;32m    262\u001b[0m     examples \u001b[39m=\u001b[39m preprocess_data(\n\u001b[1;32m    263\u001b[0m         text_a, text_b, labels, tokenizer, args\u001b[39m.\u001b[39mmax_seq_length\n\u001b[1;32m    264\u001b[0m     )\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m output_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(labels, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong)\n\u001b[1;32m    268\u001b[0m \u001b[39melif\u001b[39;00m output_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    269\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(labels, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional model configuration\n",
    "\n",
    "# train_df = pd.read_csv('data/train_MUSTARD.csv')  \n",
    "# val_df = pd.read_csv('data/val_MUSTARD.csv')\n",
    "\n",
    "train_df = train_df.head(5)\n",
    "val_df = val_df.head(5)\n",
    "model_args = {\n",
    "    'data_dir': 'data/',\n",
    "    'output_dir': 'outputs/',\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'max_seq_length': 512,\n",
    "    'output_mode': 'classification',\n",
    "    'train_batch_size': 12,\n",
    "    'eval_batch_size': 12,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 1,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-5,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_ratio': 0.06,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'evaluate_during_training': False,\n",
    "    'save_steps': 1000,\n",
    "    'eval_all_checkpoints': True,\n",
    "    'overwrite_output_dir': False,\n",
    "    'reprocess_input_data': True,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel(\n",
    "    \"roberta\", \"roberta-base\", args=model_args, use_cuda=False\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df)\n",
    "\n",
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('irony')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d7a21e848b3cc955e3fd9b39a5d790591b1d409724a3e1c3d6c5da737ac6797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
